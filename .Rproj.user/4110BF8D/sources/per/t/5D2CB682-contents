---
title: <center>Emotional Expression and Music</center>
author: "Garrett Lucas"
date: "5/2/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("tidyr")
library("ggplot2")
library("dplyr")
library("knitr")
```

### <u>Introduction</u>

```{r}
load(file = "design_matrix.RData")
table <- x

load(file = "mean_emotion_ratings.RData")
means_table <- x
```

This data was obtained from a series of experiments conducted by *Annaliese Micallef Grimaud* and *Tuomas Eerola* in 2022. These studies served the sole purpose of allowing a better understanding of how musical cues can be altered in order to convey specific emotions to a listener.

They conducted a series of three seperate experiments:

- 28 musical pieces were played to listeners, who then had to rate which emotions were portrayed by the pieces on a likert scale.

- Participants were allowed to modify different aspects of musical cues (tempo, mode, articulation, pitch, dynamics, and brightness) in order to convey the different emotions (anger, sadness, fear, joy, surprise, calmness, and power)

- Participants listened to all musical cues from the second experiment and compared them to the original pieces prior to editing. They were asked to rate them similarly to experiment 1.

New musical stimuli were created specifically for this series of experiments, as to prevent any form of familiarity bias. This also allowed for the ability to ensure that the music could be made in a way where the different aspects of the cues could be altered in experiment 2. This original music creation also allowed for a broader range of emotion to be covered, as many songs in the popular domain weren't expected to cover as wide of a variety of emotions.

This study also ensured to cover polyphonic properties of it's musical cues instead of only focusing on one aspect of said cues. This can help ensure that multiple aspects of the musical cue's makeup are considered throughout the experiment as opposed to just examining one dimension.

Below is an example of the data collected from the experiment relating to how each individual tone was altered in order to convey a specific type of emotion.

```{r}
ktable <- table |>
  head(10)
kable(ktable)
```

And here is an example of the proportions of reported emotional responses from each song above:

```{r}
kmeans_table <- means_table |>
  head(10)
kable(kmeans_table)
```

As a result of this study, it was found that mode and tempo were the most effective at shaping emotional cues and sadness/joy are the most commonly recognized emotions. Information on the dynamics and brightness categories proved to be lacking in information from the study, but it was found that soft dynamics were effective at portraying anger.

```{r}
combined_table <- left_join(table,means_table)
kombined_table <- combined_table|>
  head(10)
  kable(kombined_table)
```

This table combines both into one and makes it easier to compare the songs and which kinds of emotions were mainly elicited in the participants after listening to them.

### <u>Personal Alterations</u>

After analysing the data, I thought it would be interesting to organize it based on which aspects of a musical cue would apply most to creating a *happy*, *sad*, *peaceful*, or *scared* emotional response in the listener.Starting with the graph below, is an example of the first 10 musical cues in the list where participants listed it as having a happy tone (being above a 3.000).

```{r}
happy_table <- combined_table |>
  filter(Happy > "3")|>
  head(10)
  kable(happy_table)
```

As you may have noticed from the graph, happiness is not always the only emotion that came to people's minds when they heard these songs, and in fact, peacefulness was generally the second-highest rating for them. It would make sense, as the two emotions can be similar in effect, and may be provoked by similar sounding musical cues.Now, not every song listed above was intended to be a happy song, the chart above just means that listeners rated it highly as sounding "Happy".

Based on this chart, many of the songs vary, but one of the most notable feats is that the tempo is higher on average in songs classified as happy than it is in some of the other genres of songs. The songs classified as dominantly "happy" also tend to have a register rating of 4 or 5.

This next graph will look into the comparisons specifically for the first ten songs that have a scary rating that is above 3.

```{r}
scary_table <- combined_table |>
  filter(Scary > "3")|>
  head(10)
  kable(scary_table)
```

In this chart, it seems as though the musical cues that were considered "scary" mostly had a slower tempo, with elongated articulations. This means that when music was considered scary, the beat would be slow, and the notes would be long. Due to this, scary songs and happy songs rarely have any sort of intersection due to how differently they are, both as emotions and in how their musical cues can be interpreted.

For the next category, we will see how peacful relates to the previous two graphs:

```{r}
peaceful_table <- combined_table |>
  filter(Peaceful > "3")|>
  head(10)
  kable(peaceful_table)
```
Peaceful music is very interesting on this list, as though it has many similarities to happy music, it actually seems to have a greater consistency with what to expect. Almost every number (apart from the register) is consistently on the lower end of the spectrum. Music that is considered peaceful tends to have longer notes, a slower tempo, and gentle melodies and sound levels. 

While both happy and peaceful music share similar registers, they are actually very different. This makes sense as well, considering happy music is most likely considered very *bouncy*, while peaceful music wouldn't extend into that realm. On the other hand, happy music can also calm down to be considered peaceful as well, which is where I assume most of the similarities in the experiment stem from.

Finally, we will take a look at the songs that are considered sad:

```{r}
sad_table <- combined_table |>
  filter(Sad > "3")|>
  head(10)
  kable(sad_table)
```

Sad music seems to have much in common with both calm and scary music. It seems to be the midpoint between the two, as it doesn't tend to have high sound levels, or a fast tempo. It does however, tend to have longer notes, with a lower mode and register.

### <u>Conclusion</u>

The purpose of the study was to better understand emotions and how musicians tend to convey emotions through the music they compose. It showed that certain emotions (such as happiness and sadness), tend to be more easily captured in music while others (like power and longing) are not as easy for an audience to pick up on. 

Through this analysis, we looked at the specific emotions that music can more easily instill in a listener, and which aspects of songs may cause listeners to feel a specific way. Though there is no real explanation yet as to how music is able to convey these feelings to the human mind.

Despite this, it is still amazing to have even the slightest understanding as to what small changes could be made to a song, that could vastly change the emotional effect it has on an audience:

- Take a scary song and alter the register and sound level to transform it into something sad.
- Take a sad melody and lower the articulation to make it more peaceful.
- Even a peaceful song is practically already a happy song, all you need to do it increase the tempo.

